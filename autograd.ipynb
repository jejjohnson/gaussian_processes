{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes with Autogradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Gaussian processes are a very popular machine learning algorithm for regression. They have useful properties due to the having flexible kernels as well as access to the derivative. However, they can be a bit difficult to program because there is an optimization step. In either case, the autogradient function should be useful in many aspects and I will attempt to use that functionality for the following parts of the GP Algorithm:\n",
    "* Predictive Mean\n",
    "* Negative Log Maximum Likelihood\n",
    "* Predictive Variance\n",
    "\n",
    "My code will be a rough representation of a few repos:\n",
    "\n",
    "* Sklearn - [GaussianProcessRegressor](https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/gaussian_process/gpr.py)\n",
    "* AMOGAPE - [AMOGAPE](https://github.com/dhsvendsen/AMOGAPE/blob/master/mintGP.py)\n",
    "* Autograd - [gaussian process](https://github.com/geostk/autograd/blob/master/examples/gaussian_process.py)\n",
    "\n",
    "I will have a class implementation based on the scikit-learn framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd\n",
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.gaussian_process.kernels import (_check_length_scale)\n",
    "from sklearn.metrics.pairwise import check_pairwise_arrays\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Function (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-38d366d1c38a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-38d366d1c38a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def rbf_covariance(\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def rbf_covariance("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPRegressor(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, kernel=None, jitter=1e-09, normalize_y=False, \n",
    "#                  random_state=None):\n",
    "#         self.kernel = kernel\n",
    "#         self.jitter = jitter\n",
    "#         self.normalize_y = normalize_y\n",
    "#         self.random_state = random_state\n",
    "        \n",
    "#     def fit(self, X, y):\n",
    "        \n",
    "#         # Get the kernel function\n",
    "#         if self.kernel is None:\n",
    "#             self.kernel_ = ConstantKernel(1.0, constant_value_bounds='fixed') \\\n",
    "#                 * RBF(1.0, length_scale_bounds='fixed') \\\n",
    "#                 + WhiteKernel(0.01, length_scale_bounds='fixed')\n",
    "        \n",
    "#         # Standard Checks for inputs\n",
    "#         X, y = check_X_y(X, y, multi_output=True, y_numeric=True)\n",
    "        \n",
    "#         if self.normalize_y:\n",
    "#             # get mean of labels\n",
    "#             self.y_train_mean = np.mean(y, axis=0)\n",
    "            \n",
    "#             # remove the mean from y\n",
    "#             y -= self.y_train_mean\n",
    "#         else:\n",
    "#             self.y_train_mean = np.zeros(1)\n",
    "        \n",
    "        \n",
    "#         # -------------------------------------------\n",
    "#         # Gradient Descent\n",
    "#         # -------------------------------------------\n",
    "        \n",
    "#         return self\n",
    "    \n",
    "#     def log_marginal_likelihood(self, theta=None, eval_gradient=False):\n",
    "#         \"\"\"Returns the log-marginal likelihood of theta for the training data.\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         theta : array-like, shape = (n_kernel_params)\n",
    "#             The kernel params for the log-marginal likelihood is evaluted. \n",
    "            \n",
    "#         eval_gradient : bool, default: False\n",
    "#             If true, the gradient of the log-marginal likelihood with respect to the\n",
    "#             kernel hyperparameters at the position theta is returned additionally.\n",
    "#             If true, theta must not be none.\n",
    "            \n",
    "#         Returns\n",
    "#         -------\n",
    "#         log_likelihood : float\n",
    "#             Log-marginal likelihood of theta for training data\n",
    "        \n",
    "#         log_likelihood_gradient : array, shape = (n_kernel_params,), optional\n",
    "#             Gradient of the log-marginal likelihood with respect to the kernel \n",
    "#             hyperparameters at position theta.\n",
    "#             Only returned when eval_gradient is True.\n",
    "#         \"\"\"\n",
    "#         if theta is None:\n",
    "#             if eval_gradient:\n",
    "#                 raise ValueError(\n",
    "#                     \"Gradient can only be evaluated for theta!=None.\"\n",
    "#                 )\n",
    "#             return self.log_marginal_likelihood_value_\n",
    "        \n",
    "#         kernel = self.kernel_.clone_with_theta(theta)\n",
    "        \n",
    "#         if eval_gradient:\n",
    "#             K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
    "#         else:\n",
    "#             K = kernel(self.X_train)\n",
    "            \n",
    "#         K[np.diag_indices_from(K)] += self.alpha\n",
    "        \n",
    "#         try:\n",
    "#             L = cholesky(K, lower=True)\n",
    "#         except np.linalg.LinAlgError:\n",
    "#             return (-np.inf, np.zeros_like(theta)) if eval_gradient else -np.inf\n",
    "        \n",
    "#         # multidimensional output of self.y_train_\n",
    "#         y_train = self.y_train_\n",
    "#         if y_train.ndim == 1:\n",
    "#             y_train = y_train[:, np.newaxis]\n",
    "            \n",
    "#         # -------------------------------------\n",
    "#         # Marginal Log Likelihood Calculation\n",
    "#         # -------------------------------------\n",
    "#         alpha = cho_solve((L, true), y_train)\n",
    "        \n",
    "#         log_likelihood_dims = -0.5 * np.einsum(\"ik,ik->k\", y_train, alpha) # term 1\n",
    "#         log_likelihood_dims -= np.log(np.diag(L)).sum()                    # term 2\n",
    "#         log_likelihood_dims -= (K.shape[0] / 2) * np.log(2 * np.pi)        # term 3\n",
    "#         log_likelihood = log_likelihood_dims.sum(-1)\n",
    "        \n",
    "#         if eval_gradient:\n",
    "#             tmp = np.einsum(\"ik,jk->ijk\", alpha, alpha)  # k:output-dimension\n",
    "#             tmp -= cho_solve((L, True), np.eye(K.shape[0]))[:, :, np.newaxis]\n",
    "            \n",
    "#             log_likelihood_gradient_dims = \\\n",
    "#                 0.5 * np.einsum(\"ijl,ijk->kl\", tmp, K_gradient)\n",
    "            \n",
    "#             log_likelihood_gradient = log_likelihood_gradient_dims.sum(-1)\n",
    "            \n",
    "#             return log_likelihood, log_likelihood_gradient\n",
    "#         else:\n",
    "#             return log_likelihood\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if eval_gradient:\n",
    "#             return log_likelihood, log_likelihood_gradient\n",
    "#         else:\n",
    "#             return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.scipy.stats.multivariate_normal as mvn\n",
    "\n",
    "class GaussianProcessRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, kernel='rbf', jitter=1e-09, random_state=None):\n",
    "        self.kernel = kernel\n",
    "        self.jitter = jitter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # standardize the data\n",
    "        self.X = X\n",
    "        self.Y = y\n",
    "        \n",
    "        # initialize parameters\n",
    "        noise_scale = 0.01\n",
    "        length_scale = 10.0\n",
    "        theta0 = np.array([noise_scale, length_scale])\n",
    "        bounds = ((1e-10, 1e10), (1e-10, 1e10))\n",
    "        \n",
    "        # define objective function: negative log marginal likelihood\n",
    "        objective = lambda theta0: -self.log_marginal_likelihood(theta0)\n",
    "        \n",
    "        # minimize negative log marginal likelihood\n",
    "        cov_params = minimize(value_and_grad(objective), theta0, jac=True, args=(), method='L-BFGS-B', bounds=bounds)\n",
    "        \n",
    "        # get params\n",
    "        self.noise_scale, self.length_scale = self._unpack_kernel_params(cov_params.x)\n",
    "        \n",
    "        # calculate the weights\n",
    "        K = self.K(self.X, length_scale=self.length_scale)\n",
    "        self.L = np.linalg.cholesky(K + noise_scale * np.eye(K.shape[0]))\n",
    "        weights =  np.linalg.solve(self.L.T, np.linalg.solve(self.L, self.Y))\n",
    "        self.weights_ = weights\n",
    "        \n",
    "        return self\n",
    "    \n",
    "#     def K(self, X, Y=None, length_scale=1.0, scale=1.0):\n",
    "        \n",
    "#         scale_term = - 0.5 / np.power(length_scale, 2)\n",
    "        \n",
    "#         dists\n",
    "\n",
    "#         if Y is None:\n",
    "\n",
    "#             dists = pdist(X, metric='sqeuclidean')\n",
    "\n",
    "#             K = np.exp(scale_term * dists)\n",
    "\n",
    "#             K = squareform(K)\n",
    "\n",
    "#             np.fill_diagonal(K, 1)\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             dists = cdist(X, Y, metric='sqeuclidean')\n",
    "\n",
    "#             K = np.exp(scale_term  * dists) \n",
    "\n",
    "\n",
    "#         return K\n",
    "    def K(self, X, Y=None, length_scale=1.0, scale=1.0):\n",
    "        \n",
    "#         scale_term = - 0.5 / np.power(length_scale, 2)\n",
    "        \n",
    "        diffs = np.expand_dims(X / length_scales, 1) \\\n",
    "            - np.expanddims(Y / length_scales, 0)\n",
    "\n",
    "\n",
    "        return np.exp(-0.5 * np.sum(diffs**2, axis=2))\n",
    "    \n",
    "    def predict(self, X, return_std=None):\n",
    "        \n",
    "        K = self.K(X, self.X, length_scale=self.length_scale)\n",
    "        \n",
    "        predictions = np.dot(K, self.weights_)\n",
    "        if not return_std:\n",
    "            return predictions\n",
    "        else:            \n",
    "            K_test = rbf_covariance(X, length_scale=self.length_scale)\n",
    "            v = np.linalg.solve(self.L, K.T)\n",
    "            std_dev = np.sqrt(self.noise_scale + np.diag(K_test - np.dot(v.T, v)))\n",
    "            return predictions, std_dev\n",
    "    \n",
    "    def pred_grad(self, X):\n",
    "        \n",
    "        mu = lambda x: self.predict(x, return_std=False)\n",
    "        auto_grad = autograd.grad(mu)\n",
    "        \n",
    "        return auto_grad(X)\n",
    "    \n",
    "    def _unpack_kernel_params(self, params):\n",
    "        return params[0], params[1:]\n",
    "    \n",
    "    def log_marginal_likelihood(self, params):\n",
    "        \n",
    "        x_train = self.X\n",
    "        y_train = self.Y\n",
    "        \n",
    "        # unpack the parameters\n",
    "        noise_scale, length_scale = self._unpack_kernel_params(params)\n",
    "        \n",
    "        # calculate the covariance matrix\n",
    "        K = self.K(x_train, length_scale=length_scale)\n",
    "        K_chol = K + noise_scale * np.eye(K.shape[0])\n",
    "#         K += self.jitter * np.eye(K.shape[0])\n",
    "        \n",
    "        # Solve the cholesky\n",
    "        print(K.shape)\n",
    "        try:\n",
    "            self.L = np.linalg.cholesky(K_chol)\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            return -np.inf\n",
    "                \n",
    "        if y_train.ndim == 1:\n",
    "            y_train = y_train[:, np.newaxis]\n",
    "        \n",
    "        # get the weights\n",
    "        alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, y_train))\n",
    "        \n",
    "        # compute log-likelihood\n",
    "        log_likelihood_dims = -0.5 * np.einsum('ik,ik->k', y_train, alpha)\n",
    "        log_likelihood_dims -= np.log(np.diag(self.L)).sum()\n",
    "        log_likelihood_dims -= (K.shape[0] / 2 ) * np.log(2 * np.pi)\n",
    "        \n",
    "        log_likelihood = log_likelihood_dims.sum(-1)\n",
    "        \n",
    "        return log_likelihood\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.scipy.stats.multivariate_normal as mvn\n",
    "\n",
    "class GaussianProcessRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, kernel='rbf', jitter=1e-09, random_state=None):\n",
    "        self.kernel = kernel\n",
    "        self.jitter = jitter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # standardize the data\n",
    "        self.X = X\n",
    "        self.Y = y\n",
    "        \n",
    "        # initialize parameters\n",
    "        noise_scale = 0.01\n",
    "        length_scale = 10.0\n",
    "        theta0 = np.array([noise_scale, length_scale])\n",
    "        bounds = ((1e-10, 1e10), (1e-10, 1e10))\n",
    "        \n",
    "        # define objective function: negative log marginal likelihood\n",
    "        objective = lambda theta0: -self.log_marginal_likelihood(theta0)\n",
    "        \n",
    "        # minimize negative log marginal likelihood\n",
    "        cov_params = minimize(value_and_grad(objective), theta0, jac=True, args=(), method='L-BFGS-B', bounds=bounds)\n",
    "        \n",
    "        # get params\n",
    "        self.noise_scale, self.length_scale = self._unpack_kernel_params(cov_params.x)\n",
    "        \n",
    "        # calculate the weights\n",
    "        K = self.K(self.X, length_scale=self.length_scale)\n",
    "        self.L = np.linalg.cholesky(K + noise_scale * np.eye(K.shape[0]))\n",
    "        weights =  np.linalg.solve(self.L.T, np.linalg.solve(self.L, self.Y))\n",
    "        self.weights_ = weights\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def K(self, X, Y=None, length_scale=1.0, scale=1.0):\n",
    "        \n",
    "        scale_term = - 0.5 / np.power(length_scale, 2)\n",
    "\n",
    "        if Y is None:\n",
    "\n",
    "            dists = pdist(X, metric='sqeuclidean')\n",
    "\n",
    "            K = np.exp(scale_term * dists)\n",
    "\n",
    "            K = squareform(K)\n",
    "\n",
    "            np.fill_diagonal(K, 1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            dists = cdist(X, Y, metric='sqeuclidean')\n",
    "\n",
    "            K = np.exp(scale_term  * dists) \n",
    "\n",
    "\n",
    "        return K\n",
    "    \n",
    "    def predict(self, X, return_std=None):\n",
    "        \n",
    "        K = self.K(X, self.X, length_scale=self.length_scale)\n",
    "        \n",
    "        predictions = np.dot(K, self.weights_)\n",
    "        if not return_std:\n",
    "            return predictions\n",
    "        else:            \n",
    "            K_test = rbf_covariance(X, length_scale=self.length_scale)\n",
    "            v = np.linalg.solve(self.L, K.T)\n",
    "            std_dev = np.sqrt(self.noise_scale + np.diag(K_test - np.dot(v.T, v)))\n",
    "            return predictions, std_dev\n",
    "    \n",
    "    def pred_grad(self, X):\n",
    "        \n",
    "        mu = lambda x: self.predict(x, return_std=False)\n",
    "        auto_grad = autograd.grad(mu)\n",
    "        \n",
    "        return auto_grad(X)\n",
    "    \n",
    "    def _unpack_kernel_params(self, params):\n",
    "        return params[0], params[1:]\n",
    "    \n",
    "    def log_marginal_likelihood(self, params):\n",
    "        \n",
    "        x_train = self.X\n",
    "        y_train = self.Y\n",
    "        \n",
    "        # unpack the parameters\n",
    "        noise_scale, length_scale = self._unpack_kernel_params(params)\n",
    "        ktrain = self.K(x_train, length_scale=length_scale) \n",
    "        white_kern = noise_scale * np.eye(len(y_train))\n",
    "        print(ktrain.shape, white_kern.shape)\n",
    "        K = ktrain + white_kern\n",
    "#         # calculate the covariance matrix\n",
    "#         K = self.K(x_train, length_scale=length_scale)\n",
    "#         K_chol = K + noise_scale * np.eye(K.shape[0])\n",
    "# #         K += self.jitter * np.eye(K.shape[0])\n",
    "        \n",
    "#         # Solve the cholesky\n",
    "#         print(K.shape)\n",
    "#         try:\n",
    "#             self.L = np.linalg.cholesky(K_chol)\n",
    "            \n",
    "#         except np.linalg.LinAlgError:\n",
    "#             return -np.inf\n",
    "                \n",
    "#         if y_train.ndim == 1:\n",
    "#             y_train = y_train[:, np.newaxis]\n",
    "        \n",
    "#         # get the weights\n",
    "#         alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, y_train))\n",
    "        \n",
    "#         # compute log-likelihood\n",
    "#         log_likelihood_dims = -0.5 * np.einsum('ik,ik->k', y_train, alpha)\n",
    "#         log_likelihood_dims -= np.log(np.diag(self.L)).sum()\n",
    "#         log_likelihood_dims -= (K.shape[0] / 2 ) * np.log(2 * np.pi)\n",
    "        \n",
    "#         log_likelihood = log_likelihood_dims.sum(-1)\n",
    "        tmp = mvn.logpdf(y_train, 0.0, Kernel)\n",
    "        print(tmp)\n",
    "        return tmp\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training data: (30, 1)\n",
      "X testing data: (30, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some toy data\n",
    "d_dimensions = 1\n",
    "n_samples = 20\n",
    "noise_std = 0.1\n",
    "seed = 123\n",
    "rs = np.random.RandomState(seed)\n",
    "\n",
    "n_train = 30\n",
    "n_test = 30\n",
    "xtrain = np.linspace(-4, 5, n_train).reshape(n_train, 1)\n",
    "xtest = np.linspace(-4, 5, n_test).reshape(n_test, 1)\n",
    "print('X training data:', xtrain.shape)\n",
    "print('X testing data:', xtest.shape)\n",
    "\n",
    "\n",
    "# Labels\n",
    "f = lambda x: np.sin(x) * np.exp(0.2 * x)\n",
    "ytrain = f(xtrain) + noise_std * np.random.randn(n_train, 1)\n",
    "ytest = f(xtest)\n",
    "# Plot the function\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "ax.scatter(xtrain, ytrain)\n",
    "ax.plot(xtest, ytest)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30) (30, 30)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_trace",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-af11e02b2874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fit the gp model to the inputs and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d4993c7e166c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# minimize negative log marginal likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mcov_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L-BFGS-B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# get params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 603\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mvalue_and_grad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"Returns a function that returns both value and gradient. Suitable for use\n\u001b[1;32m    128\u001b[0m     in scipy.optimize\"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVJPNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mend_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstart_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mend_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_box\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstart_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36munary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0msubargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d4993c7e166c>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(theta0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# define objective function: negative log marginal likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_marginal_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# minimize negative log marginal likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d4993c7e166c>\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mwhite_kern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhite_kern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mktrain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwhite_kern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;31m#         # calculate the covariance matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#         K = self.K(x_train, length_scale=length_scale)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/numpy/numpy_boxes.py\u001b[0m in \u001b[0;36m__radd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__matmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__radd__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m     \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mparents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0margnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m   \u001b[0;32min\u001b[0m \u001b[0mboxed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_wrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/numpy/numpy_boxes.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__neg__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mboxed_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_constructor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_boxed_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mboxed_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0margvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxed_args\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sci_py36/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mfind_top_boxed_args\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mtop_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: _trace"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "gp_model = GaussianProcessRegressor(random_state=seed)\n",
    "\n",
    "# fit the gp model to the inputs and targets\n",
    "gp_model.fit(xtrain, ytrain)\n",
    "\n",
    "y_pred, sigma = gp_model.predict(xtest, return_std=True)\n",
    "# print(mu_der)\n",
    "# tmp = y_pred.astype()\n",
    "# print(tmp)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(xtrain, ytrain)\n",
    "ax.scatter(xtrain, ytrain)\n",
    "ax.plot(xtest, y_pred)\n",
    "ax.plot(xtest, mu_der)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianProcessRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0d5eb35a2838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fit the gp model to the inputs and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianProcessRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "gp_model = GaussianProcessRegressor(random_state=seed)\n",
    "\n",
    "# fit the gp model to the inputs and targets\n",
    "gp_model.fit(xtrain, ytrain)\n",
    "\n",
    "\n",
    "mu = lambda x: gp_model.predict(x)\n",
    "y_pred = mu(xtest)\n",
    "print(y_pred.shape)\n",
    "auto_grad = autograd.grad(mu)\n",
    "\n",
    "tmp = auto_grad(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.772588722239781\n",
      "Trained loss: 1.067270675787016\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np   # Thinly-wrapped version of Numpy\n",
    "from autograd import grad\n",
    "\n",
    "def taylor_sine(x):  # Taylor approximation to sine function\n",
    "    ans = currterm = x\n",
    "    i = 0\n",
    "    while np.abs(currterm) > 0.001:\n",
    "        currterm = -currterm * x**2 / ((2 * i + 3) * (2 * i + 2))\n",
    "        ans = ans + currterm\n",
    "        i += 1\n",
    "    return ans\n",
    "\n",
    "grad_sine = grad(taylor_sine)\n",
    "print(\"Gradient of sin(pi) is\", grad_sine(np.pi))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function unary_to_nary.<locals>.nary_operator.<locals>.nary_f at 0x7f6f8ecac0d0>\n"
     ]
    }
   ],
   "source": [
    "print(training_gradient_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.numpy.linalg import solve\n",
    "import autograd.scipy.stats.multivariate_normal as mvn\n",
    "from autograd import value_and_grad\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def make_gp_funs(cov_func, num_cov_params):\n",
    "    \"\"\"Functions that perform Gaussian process regression.\n",
    "       cov_func has signature (cov_params, x, x')\"\"\"\n",
    "\n",
    "    def unpack_kernel_params(params):\n",
    "        mean        = params[0]\n",
    "        cov_params  = params[2:]\n",
    "        noise_scale = np.exp(params[1]) + 0.0001\n",
    "        return mean, cov_params, noise_scale\n",
    "\n",
    "    def predict(params, x, y, xstar):\n",
    "        \"\"\"Returns the predictive mean and covariance at locations xstar,\n",
    "           of the latent function value f (without observation noise).\"\"\"\n",
    "        mean, cov_params, noise_scale = unpack_kernel_params(params)\n",
    "        cov_f_f = cov_func(cov_params, xstar, xstar)\n",
    "        cov_y_f = cov_func(cov_params, x, xstar)\n",
    "        cov_y_y = cov_func(cov_params, x, x) + noise_scale * np.eye(len(y))\n",
    "        pred_mean = mean +   np.dot(solve(cov_y_y, cov_y_f).T, y - mean)\n",
    "        pred_cov = cov_f_f - np.dot(solve(cov_y_y, cov_y_f).T, cov_y_f)\n",
    "        return pred_mean, pred_cov\n",
    "\n",
    "    def log_marginal_likelihood(params, x, y):\n",
    "        mean, cov_params, noise_scale = unpack_kernel_params(params)\n",
    "        cov_y_y = cov_func(cov_params, x, x) + noise_scale * np.eye(len(y))\n",
    "        prior_mean = mean * np.ones(len(y))\n",
    "        log_max = mvn.logpdf(y, prior_mean, cov_y_y)\n",
    "        print(log_max)\n",
    "        return log_max\n",
    "\n",
    "    return num_cov_params + 2, predict, log_marginal_likelihood\n",
    "\n",
    "# Define an example covariance function.\n",
    "def rbf_covariance(kernel_params, x, xp):\n",
    "    output_scale = np.exp(kernel_params[0])\n",
    "    lengthscales = np.exp(kernel_params[1:])\n",
    "    diffs = np.expand_dims(x /lengthscales, 1)\\\n",
    "          - np.expand_dims(xp/lengthscales, 0)\n",
    "    return output_scale * np.exp(-0.5 * np.sum(diffs**2, axis=2))\n",
    "\n",
    "\n",
    "def build_toy_dataset(D=1, n_data=20, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs  = np.concatenate([np.linspace(0, 3, num=n_data/2),\n",
    "                              np.linspace(6, 8, num=n_data/2)])\n",
    "    targets = (np.cos(inputs) + rs.randn(n_data) * noise_std) / 2.0\n",
    "    inputs = (inputs - 4.0) / 2.0\n",
    "    inputs  = inputs.reshape((len(inputs), D))\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def callback(params):\n",
    "    print(\"Log likelihood {}\".format(-objective(params)))\n",
    "    plt.cla()\n",
    "\n",
    "    # Show posterior marginals.\n",
    "    plot_xs = np.reshape(np.linspace(-7, 7, 300), (300,1))\n",
    "    pred_mean, pred_cov = predict(params, X, y, plot_xs)\n",
    "    marg_std = np.sqrt(np.diag(pred_cov))\n",
    "    ax.plot(plot_xs, pred_mean, 'b')\n",
    "    ax.fill(np.concatenate([plot_xs, plot_xs[::-1]]),\n",
    "            np.concatenate([pred_mean - 1.96 * marg_std,\n",
    "                           (pred_mean + 1.96 * marg_std)[::-1]]),\n",
    "            alpha=.15, fc='Blue', ec='None')\n",
    "\n",
    "    # Show samples from posterior.\n",
    "    rs = npr.RandomState(0)\n",
    "    sampled_funcs = rs.multivariate_normal(pred_mean, pred_cov, size=10)\n",
    "    ax.plot(plot_xs, sampled_funcs.T)\n",
    "\n",
    "\n",
    "    ax.plot(X, y, 'kx')\n",
    "    ax.set_ylim([-1.5, 1.5])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.draw()\n",
    "    plt.pause(1.0/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing covariance parameters...\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -31.027858408194493\n",
      "Autograd ArrayBox with value -23.16615895604356\n",
      "Autograd ArrayBox with value -22.483631947480745\n",
      "Autograd ArrayBox with value -22.421337554524072\n",
      "Autograd ArrayBox with value -22.415624457696325\n",
      "Autograd ArrayBox with value -22.415100269737973\n",
      "Autograd ArrayBox with value -22.415052172493635\n",
      "Autograd ArrayBox with value -22.415047759279766\n",
      "Autograd ArrayBox with value -22.415047354340498\n",
      "Autograd ArrayBox with value -22.41504731718485\n",
      "Autograd ArrayBox with value -22.415047313775595\n",
      "Autograd ArrayBox with value -22.41504731346277\n",
      "Autograd ArrayBox with value -22.415047313434062\n",
      "Autograd ArrayBox with value -22.415047313431437\n",
      "Autograd ArrayBox with value -22.415047313431195\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.41504731343117\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -22.415047313431167\n",
      "Autograd ArrayBox with value -31.027858408194493\n",
      "Autograd ArrayBox with value -24.494188467131\n",
      "Autograd ArrayBox with value -22.8563112792595\n",
      "Autograd ArrayBox with value -22.511338199154356\n",
      "Autograd ArrayBox with value -22.43602572218992\n",
      "Autograd ArrayBox with value -22.41962092264273\n",
      "Autograd ArrayBox with value -22.41604443760811\n",
      "Autograd ArrayBox with value -22.41526470797404\n",
      "Autograd ArrayBox with value -22.415094710216813\n",
      "Autograd ArrayBox with value -22.415057646979744\n",
      "Autograd ArrayBox with value -22.415049566373806\n",
      "Autograd ArrayBox with value -22.415047804622617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "D = 1\n",
    "\n",
    "# Build model and objective function.\n",
    "num_params, predict, log_marginal_likelihood = \\\n",
    "    make_gp_funs(rbf_covariance, num_cov_params=D + 1)\n",
    "\n",
    "X, y = build_toy_dataset(D=D)\n",
    "objective = lambda params: -log_marginal_likelihood(params, X, y)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize covariance parameters\n",
    "rs = npr.RandomState(0)\n",
    "init_params = 0.1 * rs.randn(num_params)\n",
    "\n",
    "print(\"Optimizing covariance parameters...\")\n",
    "cov_params = minimize(value_and_grad(objective), init_params, jac=True,\n",
    "                      method='CG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xstar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-608b09163591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxstar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xstar' is not defined"
     ]
    }
   ],
   "source": [
    "print(predict(X, y, xstar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.415047313431167\n",
      "22.415047313431167\n"
     ]
    }
   ],
   "source": [
    "# Build model and objective function.\n",
    "num_params, predict, log_marginal_likelihood = \\\n",
    "    make_gp_funs(rbf_covariance, num_cov_params=D + 1)\n",
    "\n",
    "X, y = build_toy_dataset(D=D)\n",
    "objective = lambda params: -log_marginal_likelihood(params, X, y)\n",
    "\n",
    "print(objective(init_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
